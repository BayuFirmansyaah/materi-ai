<!DOCTYPE html>
<html lang="id">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Panduan Deep Learning & Computer Vision</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            margin: 0;
            padding: 0;
        }

        .layout {
            display: flex;
            min-height: 100vh;
        }

        .sidebar {
            width: 280px;
            background-color: #2c3e50;
            color: white;
            position: fixed;
            height: 100vh;
            overflow-y: auto;
            padding: 20px;
            box-shadow: 2px 0 5px rgba(0,0,0,0.1);
        }

        .sidebar-header {
            padding-bottom: 20px;
            border-bottom: 2px solid #34495e;
            margin-bottom: 20px;
        }

        .sidebar-header h2 {
            margin: 0;
            font-size: 1.4em;
            color: white;
        }

        .sidebar-nav {
            list-style: none;
            margin: 0;
            padding: 0;
        }

        .sidebar-nav li {
            margin-bottom: 5px;
        }

        .sidebar-nav a {
            display: block;
            padding: 12px 15px;
            color: #ecf0f1;
            text-decoration: none;
            border-radius: 5px;
            transition: all 0.3s ease;
            font-size: 0.95em;
        }

        .sidebar-nav a:hover {
            background-color: #34495e;
            padding-left: 20px;
        }

        .sidebar-nav a.active {
            background-color: #3498db;
            font-weight: bold;
        }

        .content-wrapper {
            margin-left: 280px;
            flex: 1;
            background-color: white;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
        }

        .page {
            min-height: 100vh;
            padding: 40px;
            page-break-after: always;
        }

        .page:last-child {
            page-break-after: auto;
        }

        h1 {
            color: #2c3e50;
            font-size: 2.5em;
            margin-bottom: 20px;
            border-bottom: 4px solid #3498db;
            padding-bottom: 10px;
        }

        h2 {
            color: #34495e;
            font-size: 2em;
            margin-top: 30px;
            margin-bottom: 15px;
            border-left: 5px solid #3498db;
            padding-left: 15px;
        }

        h3 {
            color: #555;
            font-size: 1.5em;
            margin-top: 25px;
            margin-bottom: 12px;
        }

        h4 {
            color: #666;
            font-size: 1.2em;
            margin-top: 20px;
            margin-bottom: 10px;
        }

        p {
            margin-bottom: 15px;
            text-align: justify;
        }

        ul, ol {
            margin-left: 30px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
        }

        code {
            background-color: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            color: #e74c3c;
        }

        pre {
            background-color: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            line-height: 1.5;
        }

        pre code {
            background-color: transparent;
            color: #ecf0f1;
            padding: 0;
        }

        .info-box {
            background-color: #e8f4f8;
            border-left: 5px solid #3498db;
            padding: 15px;
            margin: 20px 0;
            border-radius: 3px;
        }

        .warning-box {
            background-color: #fff3cd;
            border-left: 5px solid #ffc107;
            padding: 15px;
            margin: 20px 0;
            border-radius: 3px;
        }

        .tip-box {
            background-color: #d4edda;
            border-left: 5px solid #28a745;
            padding: 15px;
            margin: 20px 0;
            border-radius: 3px;
        }

        .cover-page {
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            text-align: center;
            background: #2c3e50;
            color: white;
        }

        .cover-page h1 {
            font-size: 3.5em;
            border: none;
            color: white;
            margin-bottom: 30px;
        }

        .cover-page .subtitle {
            font-size: 1.5em;
            margin-bottom: 40px;
            opacity: 0.9;
        }

        .cover-page .info {
            font-size: 1.1em;
            margin-top: 50px;
        }

        .toc {
            margin-top: 30px;
        }

        .toc ul {
            list-style: none;
            margin-left: 0;
        }

        .toc li {
            padding: 10px 0;
            border-bottom: 1px solid #eee;
        }

        .toc a {
            text-decoration: none;
            color: #3498db;
            font-size: 1.2em;
        }

        .toc a:hover {
            color: #2980b9;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        table th {
            background-color: #3498db;
            color: white;
            padding: 12px;
            text-align: left;
        }

        table td {
            border: 1px solid #ddd;
            padding: 10px;
        }

        table tr:nth-child(even) {
            background-color: #f9f9f9;
        }

        /* Print Styles */
        @media print {
            .sidebar {
                display: none;
            }

            .content-wrapper {
                margin-left: 0;
            }

            body {
                background-color: white;
            }

            .page {
                page-break-after: always;
                padding: 20px;
            }

            .page:last-child {
                page-break-after: auto;
            }

            pre {
                page-break-inside: avoid;
            }

            h1, h2, h3 {
                page-break-after: avoid;
            }

            .cover-page {
                background: #2c3e50;
            }
        }

        /* Responsive untuk mobile */
        @media screen and (max-width: 768px) {
            .sidebar {
                width: 100%;
                height: auto;
                position: relative;
            }

            .content-wrapper {
                margin-left: 0;
            }

            .layout {
                flex-direction: column;
            }
        }

        @page {
            margin: 2cm;
        }
    </style>
</head>
<body>
    <div class="layout">
        <!-- Sidebar Navigation -->
        <div class="sidebar">
            <div class="sidebar-header">
                <h2>Panduan Pembelajaran</h2>
                <p style="font-size: 0.9em; margin-top: 10px; opacity: 0.8;">Deep Learning & Computer Vision</p>
            </div>
            <ul class="sidebar-nav">
                <li><a href="#cover">Cover</a></li>
                <li><a href="#toc">Daftar Isi</a></li>
                <li><a href="#tensorflow">1. TensorFlow</a></li>
                <li><a href="#cnn">2. CNN</a></li>
                <li><a href="#vit">3. Vision Transformer</a></li>
                <li><a href="#opencv">4. OpenCV</a></li>
                <li><a href="#kerascv">5. KerasCV</a></li>
            </ul>
        </div>

        <!-- Main Content -->
        <div class="content-wrapper">
            <div class="container">
        <!-- Cover Page -->
        <div class="page cover-page" id="cover">
            <h1>Panduan Pembelajaran</h1>
            <div class="subtitle">Deep Learning & Computer Vision</div>
            <div class="subtitle">
                <strong>Materi:</strong><br>
                TensorFlow • CNN • Vision Transformer<br>
                OpenCV • KerasCV
            </div>
            <div class="info">
                <p>Panduan Lengkap untuk Pemula hingga Menengah</p>
                <p>© 2025</p>
            </div>
        </div>

        <!-- Table of Contents -->
        <div class="page" id="toc">
            <h1>Daftar Isi</h1>
            <div class="toc">
                <ul>
                    <li><a href="#tensorflow">1. TensorFlow - Framework Deep Learning</a></li>
                    <li><a href="#cnn">2. Convolutional Neural Networks (CNN)</a></li>
                    <li><a href="#vit">3. Vision Transformer (ViT)</a></li>
                    <li><a href="#opencv">4. OpenCV - Computer Vision Library</a></li>
                    <li><a href="#kerascv">5. KerasCV - Modern Computer Vision</a></li>
                </ul>
            </div>
            
            <div class="info-box" style="margin-top: 50px;">
                <h3>Cara Menggunakan Panduan Ini</h3>
                <ul>
                    <li>Setiap materi disajikan dalam halaman terpisah untuk kemudahan print</li>
                    <li>Kode contoh dapat langsung dicoba di environment Python Anda</li>
                    <li>Bacalah secara berurutan untuk pemahaman yang sistematis</li>
                    <li>Praktikkan setiap contoh kode untuk pemahaman yang lebih baik</li>
                </ul>
            </div>

            <div class="tip-box" style="margin-top: 30px;">
                <h3>Prasyarat</h3>
                <ul>
                    <li>Pengetahuan dasar Python</li>
                    <li>Pemahaman matematika dasar (aljabar linear, kalkulus)</li>
                    <li>Familiar dengan konsep machine learning</li>
                    <li>Python 3.8 atau lebih baru terinstall</li>
                </ul>
            </div>
        </div>

        <!-- Content pages will be added here -->
        
        <!-- Page 1: TensorFlow -->
        <div class="page" id="tensorflow">
            <h1>TensorFlow - Framework Deep Learning</h1>
            
            <h2>1.1 Pengenalan TensorFlow</h2>
            <p>
                TensorFlow adalah framework open-source yang dikembangkan oleh Google Brain Team untuk komputasi numerik dan machine learning skala besar. TensorFlow memungkinkan pengembangan model deep learning dengan mudah dan dapat dijalankan di berbagai platform (CPU, GPU, TPU, mobile, web).
            </p>

            <div class="info-box">
                <h4>Keunggulan TensorFlow:</h4>
                <ul>
                    <li><strong>Fleksibilitas tinggi</strong> - Dari penelitian hingga produksi</li>
                    <li><strong>Ekosistem lengkap</strong> - TensorFlow Lite, TensorFlow.js, TensorFlow Serving</li>
                    <li><strong>Scalability</strong> - Dapat berjalan di single device hingga cluster</li>
                    <li><strong>Komunitas besar</strong> - Dokumentasi lengkap dan support aktif</li>
                    <li><strong>Production-ready</strong> - Digunakan oleh perusahaan besar dunia</li>
                </ul>
            </div>

            <h2>1.2 Instalasi TensorFlow</h2>
            <p>Instalasi TensorFlow sangat mudah menggunakan pip. Berikut adalah cara instalasi untuk berbagai kebutuhan:</p>

            <h3>Instalasi TensorFlow CPU (Recommended untuk pemula)</h3>
            <pre><code># Instalasi TensorFlow versi terbaru
pip install tensorflow

# Atau instalasi versi spesifik
pip install tensorflow==2.15.0

# Verifikasi instalasi
python -c "import tensorflow as tf; print(tf.__version__)"</code></pre>

            <h3>Instalasi TensorFlow GPU</h3>
            <pre><code># Untuk GPU support (memerlukan CUDA dan cuDNN)
pip install tensorflow[and-cuda]

# Cek apakah GPU terdeteksi
python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"</code></pre>

            <div class="warning-box">
                <h4>⚠️ Catatan Penting:</h4>
                <ul>
                    <li>TensorFlow 2.x sudah include Keras sebagai high-level API</li>
                    <li>Untuk GPU, pastikan CUDA dan cuDNN versi yang kompatibel sudah terinstall</li>
                    <li>Disarankan menggunakan virtual environment (venv atau conda)</li>
                </ul>
            </div>

            <h2>1.3 Konsep Dasar TensorFlow</h2>

            <h3>1.3.1 Tensor - Struktur Data Fundamental</h3>
            <p>
                Tensor adalah array multi-dimensi yang merupakan struktur data dasar di TensorFlow. Tensor mirip dengan NumPy array tetapi dapat dijalankan di GPU dan mendukung automatic differentiation.
            </p>

            <pre><code>import tensorflow as tf
import numpy as np

# Membuat tensor dari list
tensor_1d = tf.constant([1, 2, 3, 4, 5])
print("1D Tensor:", tensor_1d)

# Tensor 2D (Matrix)
tensor_2d = tf.constant([[1, 2, 3],
                         [4, 5, 6]])
print("2D Tensor shape:", tensor_2d.shape)

# Tensor 3D (sering digunakan untuk gambar)
tensor_3d = tf.constant([[[1, 2], [3, 4]],
                         [[5, 6], [7, 8]]])
print("3D Tensor shape:", tensor_3d.shape)

# Operasi pada tensor
a = tf.constant([[1, 2], [3, 4]])
b = tf.constant([[5, 6], [7, 8]])

# Penjumlahan
c = tf.add(a, b)
print("Addition:", c)

# Perkalian matrix
d = tf.matmul(a, b)
print("Matrix multiplication:", d)</code></pre>

            <h3>1.3.2 Variables - Tensor yang Dapat Diubah</h3>
            <p>
                Variables digunakan untuk menyimpan dan mengupdate parameter model (weights dan biases). Berbeda dengan constant, nilai variable dapat diubah selama training.
            </p>

            <pre><code># Membuat variable
weight = tf.Variable([[1.0, 2.0], [3.0, 4.0]], name="weight")
bias = tf.Variable([0.5, 0.5], name="bias")

print("Initial weight:", weight.numpy())

# Mengubah nilai variable
weight.assign([[5.0, 6.0], [7.0, 8.0]])
print("Updated weight:", weight.numpy())

# Update dengan operasi
weight.assign_add([[1.0, 1.0], [1.0, 1.0]])
print("After add:", weight.numpy())</code></pre>

            <h3>1.3.3 GradientTape - Automatic Differentiation</h3>
            <p>
                GradientTape adalah API untuk menghitung gradien secara otomatis, yang sangat penting untuk training neural networks dengan backpropagation.
            </p>

            <pre><code># Contoh menghitung gradien
x = tf.Variable(3.0)

with tf.GradientTape() as tape:
    # Forward pass: y = x^2
    y = x ** 2

# Menghitung gradien dy/dx
dy_dx = tape.gradient(y, x)
print(f"Gradient of y=x^2 at x=3: {dy_dx.numpy()}")  # Output: 6.0

# Contoh dengan multiple variables
w = tf.Variable(2.0)
b = tf.Variable(1.0)
x = tf.constant(3.0)

with tf.GradientTape(persistent=True) as tape:
    # y = w*x + b
    y = w * x + b

# Menghitung gradien terhadap w dan b
dw = tape.gradient(y, w)
db = tape.gradient(y, b)
print(f"Gradient w.r.t w: {dw.numpy()}")  # Output: 3.0
print(f"Gradient w.r.t b: {db.numpy()}")  # Output: 1.0</code></pre>

            <h2>1.4 Arsitektur dan Ekosistem TensorFlow</h2>

            <h3>1.4.1 TensorFlow Core Components</h3>
            <table>
                <tr>
                    <th>Komponen</th>
                    <th>Fungsi</th>
                </tr>
                <tr>
                    <td><strong>tf.keras</strong></td>
                    <td>High-level API untuk membangun dan training model</td>
                </tr>
                <tr>
                    <td><strong>tf.data</strong></td>
                    <td>Pipeline untuk loading dan preprocessing data</td>
                </tr>
                <tr>
                    <td><strong>tf.function</strong></td>
                    <td>Mengkonversi Python function menjadi graph untuk performa lebih baik</td>
                </tr>
                <tr>
                    <td><strong>tf.distribute</strong></td>
                    <td>Training distributed di multiple devices</td>
                </tr>
                <tr>
                    <td><strong>TensorBoard</strong></td>
                    <td>Visualization tool untuk monitoring training</td>
                </tr>
            </table>

            <h3>1.4.2 Contoh Implementasi: Model Linear Regression</h3>
            <pre><code>import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Generate data
X_train = np.linspace(0, 10, 100)
y_train = 2 * X_train + 1 + np.random.randn(100) * 0.5

# Membuat model menggunakan Keras Sequential API
model = tf.keras.Sequential([
    tf.keras.layers.Dense(1, input_shape=(1,))
])

# Compile model
model.compile(
    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),
    loss='mse',
    metrics=['mae']
)

# Training model
history = model.fit(X_train, y_train, epochs=100, verbose=0)

# Prediksi
X_test = np.array([0, 5, 10])
predictions = model.predict(X_test)
print("Predictions:", predictions.flatten())

# Mendapatkan weight dan bias
weights, bias = model.layers[0].get_weights()
print(f"Learned weight: {weights[0][0]:.2f}")
print(f"Learned bias: {bias[0]:.2f}")</code></pre>

            <h2>1.5 TensorFlow Best Practices</h2>

            <div class="tip-box">
                <h4>Tips untuk Penggunaan TensorFlow yang Efektif:</h4>
                <ul>
                    <li><strong>Gunakan tf.keras</strong> untuk rapid prototyping dan development</li>
                    <li><strong>Gunakan tf.data</strong> untuk data pipeline yang efisien</li>
                    <li><strong>Mixed precision training</strong> untuk performa lebih cepat di GPU modern</li>
                    <li><strong>Model checkpointing</strong> untuk save progress training</li>
                    <li><strong>TensorBoard</strong> untuk monitoring dan debugging</li>
                    <li><strong>tf.function decorator</strong> untuk mengoptimalkan kode Python</li>
                </ul>
            </div>

            <h3>Contoh Best Practice: Training Pipeline Lengkap</h3>
            <pre><code>import tensorflow as tf

# 1. Data Pipeline dengan tf.data
def create_dataset(x, y, batch_size=32):
    dataset = tf.data.Dataset.from_tensor_slices((x, y))
    dataset = dataset.shuffle(buffer_size=1000)
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    return dataset

# 2. Model dengan best practices
def create_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(1)
    ])
    return model

# 3. Callbacks untuk monitoring
callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5
    ),
    tf.keras.callbacks.TensorBoard(
        log_dir='./logs',
        histogram_freq=1
    )
]

# 4. Compile dan Training
model = create_model()
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='mse',
    metrics=['mae']
)

# history = model.fit(
#     train_dataset,
#     validation_data=val_dataset,
#     epochs=100,
#     callbacks=callbacks
# )</code></pre>

            <h2>1.6 Resources dan Dokumentasi</h2>
            <ul>
                <li><strong>Official Documentation:</strong> https://www.tensorflow.org/</li>
                <li><strong>TensorFlow Tutorials:</strong> https://www.tensorflow.org/tutorials</li>
                <li><strong>TensorFlow Hub:</strong> https://tfhub.dev/ (Pre-trained models)</li>
                <li><strong>TensorFlow Blog:</strong> https://blog.tensorflow.org/</li>
                <li><strong>GitHub Repository:</strong> https://github.com/tensorflow/tensorflow</li>
            </ul>

            <div class="info-box">
                <h4>Langkah Selanjutnya:</h4>
                <p>Setelah memahami TensorFlow, Anda siap untuk mempelajari arsitektur neural network yang lebih kompleks seperti CNN (Convolutional Neural Networks) di halaman berikutnya!</p>
            </div>
        </div>

        <!-- Page 2: CNN (Convolutional Neural Networks) -->
        <div class="page" id="cnn">
            <h1>Convolutional Neural Networks (CNN)</h1>
            
            <h2>2.1 Pengenalan CNN</h2>
            <p>
                Convolutional Neural Networks (CNN) adalah arsitektur deep learning yang dirancang khusus untuk memproses data yang memiliki struktur grid, seperti gambar. CNN sangat efektif untuk computer vision tasks seperti image classification, object detection, dan image segmentation.
            </p>

            <div class="info-box">
                <h4>Keunggulan CNN untuk Image Processing:</h4>
                <ul>
                    <li><strong>Spatial hierarchy</strong> - Dapat mendeteksi fitur dari simple hingga complex</li>
                    <li><strong>Parameter sharing</strong> - Filter yang sama digunakan di seluruh gambar</li>
                    <li><strong>Translation invariance</strong> - Dapat mendeteksi objek di lokasi manapun</li>
                    <li><strong>Local connectivity</strong> - Setiap neuron hanya terhubung ke region kecil input</li>
                    <li><strong>Efisiensi komputasi</strong> - Lebih sedikit parameter dibanding fully connected</li>
                </ul>
            </div>

            <h2>2.2 Arsitektur CNN - Building Blocks</h2>

            <h3>2.2.1 Convolutional Layer</h3>
            <p>
                Layer konvolusi adalah komponen utama CNN. Layer ini menggunakan filter (kernel) untuk melakukan operasi konvolusi pada input, mengekstrak fitur-fitur penting seperti edges, textures, dan patterns.
            </p>

            <div class="info-box">
                <h4>Cara Kerja Konvolusi:</h4>
                <ol>
                    <li>Filter (kernel) bergerak sliding di atas input image</li>
                    <li>Pada setiap posisi, dilakukan element-wise multiplication</li>
                    <li>Hasil perkalian dijumlahkan menjadi satu nilai</li>
                    <li>Nilai ini menjadi satu pixel di output feature map</li>
                </ol>
            </div>

            <pre><code>import tensorflow as tf

# Contoh Convolutional Layer
model = tf.keras.Sequential([
    # Input shape: (height, width, channels)
    # Filter: 32 filters dengan ukuran 3x3
    # Activation: ReLU
    tf.keras.layers.Conv2D(
        filters=32,
        kernel_size=(3, 3),
        activation='relu',
        padding='same',
        input_shape=(224, 224, 3)
    ),
    
    # Multiple conv layers untuk extract features lebih dalam
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')
])

print(model.summary())</code></pre>

            <h4>Parameter Penting pada Conv2D:</h4>
            <table>
                <tr>
                    <th>Parameter</th>
                    <th>Penjelasan</th>
                </tr>
                <tr>
                    <td><strong>filters</strong></td>
                    <td>Jumlah filter yang akan dipelajari (output channels)</td>
                </tr>
                <tr>
                    <td><strong>kernel_size</strong></td>
                    <td>Ukuran filter (biasanya 3x3 atau 5x5)</td>
                </tr>
                <tr>
                    <td><strong>strides</strong></td>
                    <td>Step size saat filter bergerak (default: 1)</td>
                </tr>
                <tr>
                    <td><strong>padding</strong></td>
                    <td>'same': output size = input size, 'valid': no padding</td>
                </tr>
                <tr>
                    <td><strong>activation</strong></td>
                    <td>Fungsi aktivasi (relu, tanh, sigmoid, dll)</td>
                </tr>
            </table>

            <h3>2.2.2 Pooling Layer</h3>
            <p>
                Pooling layer berfungsi untuk mengurangi dimensi spatial (width & height) dari feature maps, mengurangi jumlah parameter, dan membuat model lebih robust terhadap variasi posisi objek.
            </p>

            <pre><code># Max Pooling - Mengambil nilai maksimum
tf.keras.layers.MaxPooling2D(
    pool_size=(2, 2),  # Ukuran pooling window
    strides=2          # Step size (default sama dengan pool_size)
)

# Average Pooling - Mengambil rata-rata
tf.keras.layers.AveragePooling2D(
    pool_size=(2, 2)
)

# Global Average Pooling - Rata-rata seluruh feature map
# Mengurangi feature map menjadi 1x1 per channel
tf.keras.layers.GlobalAveragePooling2D()</code></pre>

            <div class="tip-box">
                <h4>Kapan Menggunakan Pooling?</h4>
                <ul>
                    <li><strong>MaxPooling:</strong> Paling populer, mempertahankan fitur dominan</li>
                    <li><strong>AveragePooling:</strong> Untuk smooth features, jarang digunakan</li>
                    <li><strong>GlobalAveragePooling:</strong> Pengganti flatten, mengurangi overfitting</li>
                </ul>
            </div>

            <h3>2.2.3 Fully Connected Layer (Dense)</h3>
            <p>
                Setelah ekstraksi fitur dengan conv dan pooling layers, fully connected layer digunakan untuk klasifikasi final. Layer ini menghubungkan semua neuron dari layer sebelumnya.
            </p>

            <pre><code># Flatten: Mengubah feature map 3D menjadi vector 1D
tf.keras.layers.Flatten()

# Dense layers untuk klasifikasi
tf.keras.layers.Dense(512, activation='relu')
tf.keras.layers.Dropout(0.5)  # Regularization
tf.keras.layers.Dense(10, activation='softmax')  # Output layer</code></pre>

            <h3>2.2.4 Batch Normalization</h3>
            <p>
                Batch Normalization menormalkan output dari layer sebelumnya, mempercepat training dan meningkatkan stabilitas model.
            </p>

            <pre><code>tf.keras.layers.Conv2D(64, (3, 3), padding='same')
tf.keras.layers.BatchNormalization()
tf.keras.layers.Activation('relu')</code></pre>

            <h2>2.3 Arsitektur CNN Klasik</h2>

            <h3>2.3.1 LeNet-5 (1998)</h3>
            <p>Salah satu arsitektur CNN pertama untuk digit recognition.</p>
            <pre><code># LeNet-5 inspired architecture
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.AveragePooling2D((2, 2)),
    tf.keras.layers.Conv2D(16, (5, 5), activation='relu'),
    tf.keras.layers.AveragePooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(120, activation='relu'),
    tf.keras.layers.Dense(84, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])</code></pre>

            <h3>2.3.2 VGGNet (2014)</h3>
            <p>Menggunakan stacked 3x3 convolutions dengan depth yang dalam.</p>
            <pre><code># VGG-like block
def vgg_block(filters):
    return tf.keras.Sequential([
        tf.keras.layers.Conv2D(filters, (3, 3), activation='relu', padding='same'),
        tf.keras.layers.Conv2D(filters, (3, 3), activation='relu', padding='same'),
        tf.keras.layers.MaxPooling2D((2, 2))
    ])</code></pre>

            <h3>2.3.3 ResNet (2015)</h3>
            <p>Memperkenalkan skip connections untuk training network yang sangat dalam.</p>
            <pre><code># Residual Block
def residual_block(x, filters):
    # Shortcut connection
    shortcut = x
    
    # Main path
    x = tf.keras.layers.Conv2D(filters, (3, 3), padding='same')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation('relu')(x)
    
    x = tf.keras.layers.Conv2D(filters, (3, 3), padding='same')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    
    # Add shortcut
    x = tf.keras.layers.Add()([x, shortcut])
    x = tf.keras.layers.Activation('relu')(x)
    
    return x</code></pre>

            <h2>2.4 Implementasi CNN Lengkap untuk Image Classification</h2>

            <h3>Contoh: CNN untuk CIFAR-10 Dataset</h3>
            <pre><code>import tensorflow as tf
from tensorflow.keras import layers, models

# 1. Load dan Preprocess Data
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

# Normalisasi pixel values ke [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# 2. Membuat CNN Model
def create_cnn_model():
    model = models.Sequential([
        # Block 1
        layers.Conv2D(32, (3, 3), activation='relu', padding='same', 
                      input_shape=(32, 32, 3)),
        layers.BatchNormalization(),
        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # Block 2
        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # Block 3
        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # Classification Head
        layers.Flatten(),
        layers.Dense(512, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(10, activation='softmax')
    ])
    return model

model = create_cnn_model()

# 3. Compile Model
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

print(model.summary())

# 4. Data Augmentation
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
])

# 5. Callbacks
callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5),
]

# 6. Training
# history = model.fit(
#     x_train, y_train,
#     batch_size=128,
#     epochs=100,
#     validation_split=0.2,
#     callbacks=callbacks
# )

# 7. Evaluasi
# test_loss, test_acc = model.evaluate(x_test, y_test)
# print(f'Test accuracy: {test_acc:.4f}')</code></pre>

            <h2>2.5 Transfer Learning dengan Pre-trained CNN</h2>
            <p>
                Transfer learning menggunakan model yang sudah ditraining pada dataset besar (seperti ImageNet) dan mengadaptasinya untuk task spesifik Anda.
            </p>

            <pre><code># Menggunakan MobileNetV2 pre-trained
base_model = tf.keras.applications.MobileNetV2(
    input_shape=(224, 224, 3),
    include_top=False,  # Tidak include classification head
    weights='imagenet'
)

# Freeze base model
base_model.trainable = False

# Membuat model baru
model = tf.keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Fine-tuning: Setelah training, unfreeze beberapa layers
# base_model.trainable = True
# for layer in base_model.layers[:-20]:
#     layer.trainable = False</code></pre>

            <h2>2.6 CNN Best Practices</h2>

            <div class="tip-box">
                <h4>Tips untuk CNN yang Efektif:</h4>
                <ul>
                    <li><strong>Data Augmentation:</strong> Rotasi, flip, zoom untuk increase variasi data</li>
                    <li><strong>Batch Normalization:</strong> Setelah Conv2D sebelum activation</li>
                    <li><strong>Dropout:</strong> 0.25-0.5 untuk mencegah overfitting</li>
                    <li><strong>Learning Rate Scheduling:</strong> Reduce on plateau atau cosine decay</li>
                    <li><strong>Multiple Conv Layers:</strong> Stack 2-3 Conv2D sebelum pooling</li>
                    <li><strong>Global Average Pooling:</strong> Alternatif Flatten untuk reduce parameters</li>
                    <li><strong>Transfer Learning:</strong> Gunakan pre-trained models untuk dataset kecil</li>
                </ul>
            </div>

            <h2>2.7 Visualisasi Feature Maps</h2>
            <pre><code># Visualisasi feature maps dari conv layers
def visualize_feature_maps(model, image, layer_name):
    # Membuat model untuk output intermediate layer
    feature_model = tf.keras.Model(
        inputs=model.input,
        outputs=model.get_layer(layer_name).output
    )
    
    # Get feature maps
    feature_maps = feature_model.predict(image[np.newaxis, ...])
    
    # Plot feature maps
    import matplotlib.pyplot as plt
    n_features = feature_maps.shape[-1]
    size = feature_maps.shape[1]
    
    display_grid = np.zeros((size, size * n_features))
    
    for i in range(n_features):
        x = feature_maps[0, :, :, i]
        x -= x.mean()
        x /= x.std()
        x *= 64
        x += 128
        x = np.clip(x, 0, 255).astype('uint8')
        display_grid[:, i * size : (i + 1) * size] = x
    
    plt.figure(figsize=(20, 20))
    plt.imshow(display_grid, cmap='viridis')
    plt.show()</code></pre>

            <div class="info-box">
                <h4>Langkah Selanjutnya:</h4>
                <p>Setelah memahami CNN, Anda siap untuk mempelajari arsitektur modern seperti Vision Transformer yang menggunakan attention mechanism di halaman berikutnya!</p>
            </div>
        </div>

        <!-- Page 3: Vision Transformer (ViT) -->
        <div class="page" id="vit">
            <h1>Vision Transformer (ViT)</h1>
            
            <h2>3.1 Pengenalan Vision Transformer</h2>
            <p>
                Vision Transformer (ViT) adalah arsitektur yang mengadaptasi Transformer (awalnya untuk NLP) untuk computer vision. ViT membuktikan bahwa pure attention mechanism tanpa convolution dapat mencapai atau bahkan melampaui performa CNN pada image classification tasks.
            </p>

            <div class="info-box">
                <h4>Perbedaan Utama CNN vs Vision Transformer:</h4>
                <table style="margin-top: 15px;">
                    <tr>
                        <th>Aspek</th>
                        <th>CNN</th>
                        <th>Vision Transformer</th>
                    </tr>
                    <tr>
                        <td><strong>Inductive Bias</strong></td>
                        <td>Strong (locality, translation equivariance)</td>
                        <td>Minimal (hanya patch-based)</td>
                    </tr>
                    <tr>
                        <td><strong>Receptive Field</strong></td>
                        <td>Local → Global secara bertahap</td>
                        <td>Global dari awal (self-attention)</td>
                    </tr>
                    <tr>
                        <td><strong>Data Requirement</strong></td>
                        <td>Dapat bekerja dengan data lebih sedikit</td>
                        <td>Memerlukan dataset besar untuk training</td>
                    </tr>
                    <tr>
                        <td><strong>Computational Cost</strong></td>
                        <td>Lebih efisien untuk image kecil</td>
                        <td>Lebih efisien untuk image besar</td>
                    </tr>
                    <tr>
                        <td><strong>Interpretability</strong></td>
                        <td>Feature maps sulit diinterpretasi</td>
                        <td>Attention maps lebih interpretable</td>
                    </tr>
                </table>
            </div>

            <h2>3.2 Attention Mechanism - Fondasi Transformer</h2>

            <h3>3.2.1 Self-Attention Mechanism</h3>
            <p>
                Self-attention memungkinkan model untuk "memperhatikan" bagian-bagian berbeda dari input secara simultan, menangkap dependencies jarak jauh dalam satu operasi.
            </p>

            <div class="info-box">
                <h4>Cara Kerja Self-Attention:</h4>
                <ol>
                    <li><strong>Query (Q), Key (K), Value (V):</strong> Input diprojeksikan menjadi 3 representasi</li>
                    <li><strong>Attention Scores:</strong> Hitung similarity antara Q dan K</li>
                    <li><strong>Softmax:</strong> Normalize scores menjadi probability distribution</li>
                    <li><strong>Weighted Sum:</strong> Aggregate V berdasarkan attention weights</li>
                </ol>
            </div>

            <pre><code>import tensorflow as tf
import numpy as np

# Formula: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V

class SelfAttention(tf.keras.layers.Layer):
    def __init__(self, d_model):
        super(SelfAttention, self).__init__()
        self.d_model = d_model
        
        # Linear projections untuk Q, K, V
        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)
        
    def call(self, x):
        # x shape: (batch_size, seq_len, d_model)
        
        # Generate Q, K, V
        q = self.wq(x)  # (batch, seq_len, d_model)
        k = self.wk(x)  # (batch, seq_len, d_model)
        v = self.wv(x)  # (batch, seq_len, d_model)
        
        # Compute attention scores
        matmul_qk = tf.matmul(q, k, transpose_b=True)
        
        # Scale
        dk = tf.cast(tf.shape(k)[-1], tf.float32)
        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)
        
        # Softmax
        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)
        
        # Apply attention to values
        output = tf.matmul(attention_weights, v)
        
        return output, attention_weights

# Contoh penggunaan
d_model = 512
seq_len = 10
batch_size = 2

x = tf.random.normal((batch_size, seq_len, d_model))
attention_layer = SelfAttention(d_model)
output, weights = attention_layer(x)

print(f"Output shape: {output.shape}")
print(f"Attention weights shape: {weights.shape}")</code></pre>

            <h3>3.2.2 Multi-Head Attention</h3>
            <p>
                Multi-Head Attention menjalankan self-attention secara parallel dengan parameter yang berbeda, memungkinkan model untuk fokus pada berbagai aspek dari input secara bersamaan.
            </p>

            <pre><code>class MultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        
        assert d_model % num_heads == 0
        
        self.depth = d_model // num_heads
        
        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)
        
        self.dense = tf.keras.layers.Dense(d_model)
        
    def split_heads(self, x, batch_size):
        # Split last dimension into (num_heads, depth)
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])
    
    def call(self, x):
        batch_size = tf.shape(x)[0]
        
        q = self.wq(x)
        k = self.wk(x)
        v = self.wv(x)
        
        q = self.split_heads(q, batch_size)
        k = self.split_heads(k, batch_size)
        v = self.split_heads(v, batch_size)
        
        # Scaled dot-product attention
        matmul_qk = tf.matmul(q, k, transpose_b=True)
        dk = tf.cast(tf.shape(k)[-1], tf.float32)
        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)
        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)
        output = tf.matmul(attention_weights, v)
        
        # Concatenate heads
        output = tf.transpose(output, perm=[0, 2, 1, 3])
        concat_attention = tf.reshape(output, (batch_size, -1, self.d_model))
        
        output = self.dense(concat_attention)
        
        return output

# Contoh penggunaan
mha = MultiHeadAttention(d_model=512, num_heads=8)
output = mha(x)
print(f"Multi-Head Attention output shape: {output.shape}")</code></pre>

            <h2>3.3 Arsitektur Vision Transformer (ViT)</h2>

            <h3>3.3.1 Patch Embedding</h3>
            <p>
                ViT membagi gambar menjadi patch-patch kecil (misalnya 16x16 pixels), kemudian setiap patch diperlakukan seperti token dalam NLP.
            </p>

            <pre><code>class PatchEmbedding(tf.keras.layers.Layer):
    def __init__(self, image_size, patch_size, embed_dim):
        super(PatchEmbedding, self).__init__()
        self.patch_size = patch_size
        self.num_patches = (image_size // patch_size) ** 2
        
        # Convolusi untuk extract patches dan embed
        self.projection = tf.keras.layers.Conv2D(
            filters=embed_dim,
            kernel_size=patch_size,
            strides=patch_size,
            padding='valid'
        )
        
    def call(self, x):
        # x shape: (batch, height, width, channels)
        x = self.projection(x)  # (batch, H/P, W/P, embed_dim)
        
        # Flatten patches
        batch_size = tf.shape(x)[0]
        x = tf.reshape(x, (batch_size, -1, x.shape[-1]))
        
        return x

# Contoh
image_size = 224
patch_size = 16
embed_dim = 768

x = tf.random.normal((2, image_size, image_size, 3))
patch_embed = PatchEmbedding(image_size, patch_size, embed_dim)
patches = patch_embed(x)

print(f"Number of patches: {patches.shape[1]}")
print(f"Patch embedding dim: {patches.shape[2]}")</code></pre>

            <h3>3.3.2 Positional Encoding</h3>
            <p>
                Karena attention tidak memiliki informasi posisi, positional encoding ditambahkan untuk memberikan informasi tentang lokasi patch dalam gambar.
            </p>

            <pre><code>class PositionalEncoding(tf.keras.layers.Layer):
    def __init__(self, num_patches, embed_dim):
        super(PositionalEncoding, self).__init__()
        # Learnable positional embeddings
        self.pos_embedding = self.add_weight(
            name='pos_embedding',
            shape=(1, num_patches + 1, embed_dim),
            initializer='random_normal',
            trainable=True
        )
        
    def call(self, x):
        return x + self.pos_embedding</code></pre>

            <h3>3.3.3 Transformer Encoder Block</h3>
            <pre><code>class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):
        super(TransformerBlock, self).__init__()
        
        self.att = MultiHeadAttention(embed_dim, num_heads)
        self.mlp = tf.keras.Sequential([
            tf.keras.layers.Dense(mlp_dim, activation='gelu'),
            tf.keras.layers.Dropout(dropout),
            tf.keras.layers.Dense(embed_dim),
            tf.keras.layers.Dropout(dropout)
        ])
        
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout = tf.keras.layers.Dropout(dropout)
        
    def call(self, x, training):
        # Multi-Head Attention dengan residual connection
        attn_output = self.att(x)
        attn_output = self.dropout(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output)
        
        # MLP dengan residual connection
        mlp_output = self.mlp(out1, training=training)
        out2 = self.layernorm2(out1 + mlp_output)
        
        return out2</code></pre>

            <h2>3.4 Implementasi Vision Transformer Lengkap</h2>

            <pre><code>import tensorflow as tf

class VisionTransformer(tf.keras.Model):
    def __init__(
        self,
        image_size=224,
        patch_size=16,
        num_classes=1000,
        embed_dim=768,
        num_heads=12,
        num_layers=12,
        mlp_dim=3072,
        dropout=0.1
    ):
        super(VisionTransformer, self).__init__()
        
        self.patch_embed = PatchEmbedding(image_size, patch_size, embed_dim)
        num_patches = (image_size // patch_size) ** 2
        
        # Class token (seperti [CLS] token di BERT)
        self.class_token = self.add_weight(
            name='class_token',
            shape=(1, 1, embed_dim),
            initializer='random_normal',
            trainable=True
        )
        
        self.pos_embed = PositionalEncoding(num_patches, embed_dim)
        self.dropout = tf.keras.layers.Dropout(dropout)
        
        # Transformer blocks
        self.transformer_blocks = [
            TransformerBlock(embed_dim, num_heads, mlp_dim, dropout)
            for _ in range(num_layers)
        ]
        
        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        
        # Classification head
        self.head = tf.keras.layers.Dense(num_classes)
        
    def call(self, x, training=False):
        batch_size = tf.shape(x)[0]
        
        # Patch embedding
        x = self.patch_embed(x)
        
        # Prepend class token
        class_tokens = tf.broadcast_to(
            self.class_token, 
            [batch_size, 1, self.class_token.shape[-1]]
        )
        x = tf.concat([class_tokens, x], axis=1)
        
        # Add positional encoding
        x = self.pos_embed(x)
        x = self.dropout(x, training=training)
        
        # Transformer blocks
        for block in self.transformer_blocks:
            x = block(x, training=training)
        
        x = self.layernorm(x)
        
        # Classification (menggunakan class token)
        cls_token = x[:, 0]
        logits = self.head(cls_token)
        
        return logits

# Membuat ViT model
model = VisionTransformer(
    image_size=224,
    patch_size=16,
    num_classes=10,
    embed_dim=768,
    num_heads=12,
    num_layers=12,
    mlp_dim=3072
)

# Compile
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

# Test forward pass
x_test = tf.random.normal((2, 224, 224, 3))
output = model(x_test)
print(f"Output shape: {output.shape}")</code></pre>

            <h2>3.5 Pre-trained Vision Transformer Models</h2>

            <h3>3.5.1 Menggunakan ViT dari TensorFlow Hub atau Keras</h3>
            <pre><code># Menggunakan pre-trained ViT
import tensorflow_hub as hub

# Load pre-trained ViT
vit_model = hub.KerasLayer(
    "https://tfhub.dev/sayakpaul/vit_b16_fe/1",
    trainable=False
)

# Membuat model untuk fine-tuning
model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),
    vit_model,
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)</code></pre>

            <h3>3.5.2 Variasi ViT Models</h3>
            <table>
                <tr>
                    <th>Model</th>
                    <th>Patch Size</th>
                    <th>Embed Dim</th>
                    <th>Layers</th>
                    <th>Parameters</th>
                </tr>
                <tr>
                    <td><strong>ViT-Base</strong></td>
                    <td>16x16</td>
                    <td>768</td>
                    <td>12</td>
                    <td>86M</td>
                </tr>
                <tr>
                    <td><strong>ViT-Large</strong></td>
                    <td>16x16</td>
                    <td>1024</td>
                    <td>24</td>
                    <td>307M</td>
                </tr>
                <tr>
                    <td><strong>ViT-Huge</strong></td>
                    <td>14x14</td>
                    <td>1280</td>
                    <td>32</td>
                    <td>632M</td>
                </tr>
            </table>

            <h2>3.6 Hybrid Models: CNN + Transformer</h2>
            <p>
                Menggabungkan kekuatan CNN (inductive bias) dengan Transformer (global attention) untuk hasil terbaik.
            </p>

            <pre><code># Hybrid: CNN backbone + Transformer
def create_hybrid_model():
    # CNN untuk feature extraction
    backbone = tf.keras.applications.ResNet50(
        include_top=False,
        weights='imagenet',
        input_shape=(224, 224, 3)
    )
    backbone.trainable = False
    
    # Transformer untuk sequence modeling
    inputs = tf.keras.Input(shape=(224, 224, 3))
    x = backbone(inputs)
    
    # Reshape CNN output menjadi sequence
    batch_size = tf.shape(x)[0]
    x = tf.reshape(x, (batch_size, -1, x.shape[-1]))
    
    # Positional encoding
    x = PositionalEncoding(x.shape[1], x.shape[-1])(x)
    
    # Transformer blocks
    for _ in range(4):
        x = TransformerBlock(
            embed_dim=x.shape[-1],
            num_heads=8,
            mlp_dim=2048
        )(x, training=False)
    
    # Classification
    x = tf.keras.layers.GlobalAveragePooling1D()(x)
    outputs = tf.keras.layers.Dense(10, activation='softmax')(x)
    
    model = tf.keras.Model(inputs, outputs)
    return model</code></pre>

            <h2>3.7 Vision Transformer Best Practices</h2>

            <div class="tip-box">
                <h4>Tips untuk Training ViT:</h4>
                <ul>
                    <li><strong>Dataset Besar:</strong> ViT memerlukan dataset besar (>100K images) atau gunakan pre-trained</li>
                    <li><strong>Data Augmentation:</strong> Sangat penting, gunakan RandAugment atau MixUp</li>
                    <li><strong>Learning Rate:</strong> Gunakan warmup (1-10 epochs) kemudian cosine decay</li>
                    <li><strong>Regularization:</strong> Weight decay, dropout, stochastic depth</li>
                    <li><strong>Patch Size:</strong> Smaller patch = lebih banyak tokens = lebih lambat tapi lebih akurat</li>
                    <li><strong>Fine-tuning:</strong> Untuk dataset kecil, fine-tune pre-trained ViT</li>
                    <li><strong>Mixed Precision:</strong> Gunakan fp16 untuk training lebih cepat</li>
                </ul>
            </div>

            <h2>3.8 Visualisasi Attention Maps</h2>
            <pre><code># Visualisasi attention weights
def visualize_attention(model, image):
    import matplotlib.pyplot as plt
    
    # Get attention weights dari transformer blocks
    attention_model = tf.keras.Model(
        inputs=model.input,
        outputs=[layer.att for layer in model.transformer_blocks]
    )
    
    # Forward pass
    attentions = attention_model(image[np.newaxis, ...])
    
    # Plot attention maps
    fig, axes = plt.subplots(3, 4, figsize=(16, 12))
    axes = axes.flatten()
    
    for idx, attention in enumerate(attentions[:12]):
        # Attention shape: (batch, num_heads, seq_len, seq_len)
        # Ambil attention dari class token ke patches
        att_map = attention[0, :, 0, 1:].numpy()  # (num_heads, num_patches)
        att_map = att_map.mean(axis=0)  # Average across heads
        
        # Reshape ke spatial grid
        size = int(np.sqrt(att_map.shape[0]))
        att_map = att_map.reshape(size, size)
        
        axes[idx].imshow(att_map, cmap='viridis')
        axes[idx].set_title(f'Layer {idx+1}')
        axes[idx].axis('off')
    
    plt.tight_layout()
    plt.show()</code></pre>

            <div class="warning-box">
                <h4>Kapan Menggunakan ViT vs CNN?</h4>
                <ul>
                    <li><strong>Gunakan ViT:</strong> Dataset besar (>1M images), perlu global context, task kompleks</li>
                    <li><strong>Gunakan CNN:</strong> Dataset kecil-medium, real-time inference, resource terbatas</li>
                    <li><strong>Gunakan Hybrid:</strong> Balance antara efficiency dan performance</li>
                </ul>
            </div>

            <div class="info-box">
                <h4>Langkah Selanjutnya:</h4>
                <p>Setelah memahami deep learning models, mari belajar tools praktis untuk image processing dengan OpenCV di halaman berikutnya!</p>
            </div>
        </div>

        <!-- Page 4: OpenCV -->
        <div class="page" id="opencv">
            <h1>OpenCV - Open Computer Vision</h1>
            
            <h2>4.1 Pengenalan OpenCV</h2>
            <p>
                OpenCV (Open Source Computer Vision Library) adalah library open-source yang berisi lebih dari 2500 algoritma untuk computer vision dan machine learning. OpenCV sangat populer untuk real-time image processing dan dapat diintegrasikan dengan deep learning frameworks.
            </p>

            <div class="info-box">
                <h4>Keunggulan OpenCV:</h4>
                <ul>
                    <li><strong>Performa Tinggi</strong> - Optimized dengan C++ dan dapat menggunakan GPU</li>
                    <li><strong>Cross-platform</strong> - Windows, Linux, macOS, Android, iOS</li>
                    <li><strong>Lengkap</strong> - Image processing, video analysis, object detection, feature extraction</li>
                    <li><strong>Gratis dan Open Source</strong> - BSD license</li>
                    <li><strong>Community Support</strong> - Dokumentasi lengkap dan komunitas besar</li>
                    <li><strong>DNN Module</strong> - Support untuk running deep learning models</li>
                </ul>
            </div>

            <h2>4.2 Instalasi OpenCV</h2>

            <h3>4.2.1 Instalasi OpenCV Python</h3>
            <pre><code># Instalasi opencv-python (versi dasar)
pip install opencv-python

# Instalasi opencv-contrib-python (dengan extra modules)
pip install opencv-contrib-python

# Verifikasi instalasi
python -c "import cv2; print(cv2.__version__)"

# Untuk mendukung GUI (highgui)
# Windows: biasanya sudah include
# Linux: sudo apt-get install python3-opencv</code></pre>

            <div class="warning-box">
                <h4>Catatan Instalasi:</h4>
                <ul>
                    <li><code>opencv-python</code>: Main modules (cukup untuk sebagian besar use cases)</li>
                    <li><code>opencv-contrib-python</code>: Include extra modules (SIFT, SURF, etc.)</li>
                    <li><code>opencv-python-headless</code>: Tanpa GUI support (untuk server)</li>
                </ul>
            </div>

            <h2>4.3 Operasi Dasar OpenCV</h2>

            <h3>4.3.1 Loading dan Displaying Images</h3>
            <pre><code>import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load image (BGR format)
image = cv2.imread('image.jpg')

# Convert BGR to RGB (untuk matplotlib)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Display menggunakan matplotlib
plt.imshow(image_rgb)
plt.title('Original Image')
plt.axis('off')
plt.show()

# Display menggunakan OpenCV window
cv2.imshow('Image', image)
cv2.waitKey(0)  # Tunggu sampai key press
cv2.destroyAllWindows()

# Get image properties
print(f"Image shape: {image.shape}")  # (height, width, channels)
print(f"Image dtype: {image.dtype}")
print(f"Image size: {image.size} pixels")</code></pre>

            <h3>4.3.2 Color Space Conversions</h3>
            <pre><code># BGR to RGB
rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# BGR to Grayscale
gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# BGR to HSV (Hue, Saturation, Value)
hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

# BGR to LAB
lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)

# Visualisasi
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

axes[0, 0].imshow(rgb_image)
axes[0, 0].set_title('RGB')
axes[0, 0].axis('off')

axes[0, 1].imshow(gray_image, cmap='gray')
axes[0, 1].set_title('Grayscale')
axes[0, 1].axis('off')

axes[1, 0].imshow(hsv_image)
axes[1, 0].set_title('HSV')
axes[1, 0].axis('off')

axes[1, 1].imshow(lab_image)
axes[1, 1].set_title('LAB')
axes[1, 1].axis('off')

plt.tight_layout()
plt.show()</code></pre>

            <h3>4.3.3 Geometric Transformations</h3>
            <pre><code># Resize image
resized = cv2.resize(image, (300, 300))  # Width x Height
# Atau dengan scaling factor
resized_scaled = cv2.resize(image, None, fx=0.5, fy=0.5)

# Rotate image
height, width = image.shape[:2]
center = (width // 2, height // 2)
rotation_matrix = cv2.getRotationMatrix2D(center, angle=45, scale=1.0)
rotated = cv2.warpAffine(image, rotation_matrix, (width, height))

# Flip image
flipped_horizontal = cv2.flip(image, 1)  # 1: horizontal
flipped_vertical = cv2.flip(image, 0)    # 0: vertical
flipped_both = cv2.flip(image, -1)       # -1: both

# Crop image
cropped = image[100:400, 200:500]  # [y1:y2, x1:x2]

# Affine transformation
pts1 = np.float32([[50, 50], [200, 50], [50, 200]])
pts2 = np.float32([[10, 100], [200, 50], [100, 250]])
affine_matrix = cv2.getAffineTransform(pts1, pts2)
affine_transformed = cv2.warpAffine(image, affine_matrix, (width, height))</code></pre>

            <h2>4.4 Image Filtering dan Enhancement</h2>

            <h3>4.4.1 Smoothing Filters</h3>
            <pre><code># Gaussian Blur (mengurangi noise)
gaussian_blur = cv2.GaussianBlur(image, (5, 5), 0)

# Median Blur (bagus untuk salt-and-pepper noise)
median_blur = cv2.medianBlur(image, 5)

# Bilateral Filter (edge-preserving smoothing)
bilateral = cv2.bilateralFilter(image, 9, 75, 75)

# Average Blur
average_blur = cv2.blur(image, (5, 5))

# Visualisasi
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes[0, 0].imshow(cv2.cvtColor(gaussian_blur, cv2.COLOR_BGR2RGB))
axes[0, 0].set_title('Gaussian Blur')
axes[0, 1].imshow(cv2.cvtColor(median_blur, cv2.COLOR_BGR2RGB))
axes[0, 1].set_title('Median Blur')
axes[1, 0].imshow(cv2.cvtColor(bilateral, cv2.COLOR_BGR2RGB))
axes[1, 0].set_title('Bilateral Filter')
axes[1, 1].imshow(cv2.cvtColor(average_blur, cv2.COLOR_BGR2RGB))
axes[1, 1].set_title('Average Blur')
plt.tight_layout()
plt.show()</code></pre>

            <h3>4.4.2 Edge Detection</h3>
            <pre><code># Canny Edge Detection
edges_canny = cv2.Canny(gray_image, threshold1=50, threshold2=150)

# Sobel Edge Detection
sobelx = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)  # X direction
sobely = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)  # Y direction
sobel_combined = cv2.magnitude(sobelx, sobely)

# Laplacian Edge Detection
laplacian = cv2.Laplacian(gray_image, cv2.CV_64F)

# Visualisasi
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
axes[0].imshow(edges_canny, cmap='gray')
axes[0].set_title('Canny')
axes[1].imshow(sobel_combined, cmap='gray')
axes[1].set_title('Sobel')
axes[2].imshow(laplacian, cmap='gray')
axes[2].set_title('Laplacian')
plt.tight_layout()
plt.show()</code></pre>

            <h3>4.4.3 Morphological Operations</h3>
            <pre><code># Binary threshold
_, binary = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)

# Kernel untuk morphological operations
kernel = np.ones((5, 5), np.uint8)

# Erosion (mengikis boundaries)
erosion = cv2.erode(binary, kernel, iterations=1)

# Dilation (memperluas boundaries)
dilation = cv2.dilate(binary, kernel, iterations=1)

# Opening (erosion followed by dilation) - remove noise
opening = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)

# Closing (dilation followed by erosion) - fill gaps
closing = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)

# Gradient (difference between dilation and erosion)
gradient = cv2.morphologyEx(binary, cv2.MORPH_GRADIENT, kernel)</code></pre>

            <h2>4.5 Feature Detection dan Matching</h2>

            <h3>4.5.1 Corner Detection</h3>
            <pre><code># Harris Corner Detection
gray_float = np.float32(gray_image)
harris_corners = cv2.cornerHarris(gray_float, blockSize=2, ksize=3, k=0.04)
harris_corners = cv2.dilate(harris_corners, None)

# Threshold untuk menandai corners
image_corners = image.copy()
image_corners[harris_corners > 0.01 * harris_corners.max()] = [0, 0, 255]

# Shi-Tomasi Corner Detection (Good Features to Track)
corners = cv2.goodFeaturesToTrack(
    gray_image,
    maxCorners=100,
    qualityLevel=0.01,
    minDistance=10
)

image_shi_tomasi = image.copy()
for corner in corners:
    x, y = corner.ravel()
    cv2.circle(image_shi_tomasi, (int(x), int(y)), 3, (0, 255, 0), -1)</code></pre>

            <h3>4.5.2 SIFT, ORB, AKAZE Features</h3>
            <pre><code># ORB (Oriented FAST and Rotated BRIEF)
orb = cv2.ORB_create()
keypoints_orb, descriptors_orb = orb.detectAndCompute(gray_image, None)

# Draw keypoints
image_orb = cv2.drawKeypoints(
    image, 
    keypoints_orb, 
    None, 
    color=(0, 255, 0),
    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS
)

# AKAZE (lebih robust)
akaze = cv2.AKAZE_create()
keypoints_akaze, descriptors_akaze = akaze.detectAndCompute(gray_image, None)

# SIFT (memerlukan opencv-contrib-python)
# sift = cv2.SIFT_create()
# keypoints_sift, descriptors_sift = sift.detectAndCompute(gray_image, None)

print(f"Number of ORB keypoints: {len(keypoints_orb)}")
print(f"Number of AKAZE keypoints: {len(keypoints_akaze)}")</code></pre>

            <h2>4.6 Object Detection dengan OpenCV</h2>

            <h3>4.6.1 Cascade Classifier (Haar/LBP)</h3>
            <pre><code># Load pre-trained cascade classifier untuk face detection
face_cascade = cv2.CascadeClassifier(
    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
)

# Detect faces
faces = face_cascade.detectMultiScale(
    gray_image,
    scaleFactor=1.1,
    minNeighbors=5,
    minSize=(30, 30)
)

# Draw rectangles around faces
image_faces = image.copy()
for (x, y, w, h) in faces:
    cv2.rectangle(image_faces, (x, y), (x+w, y+h), (255, 0, 0), 2)
    cv2.putText(
        image_faces, 
        'Face', 
        (x, y-10),
        cv2.FONT_HERSHEY_SIMPLEX,
        0.9,
        (255, 0, 0),
        2
    )

print(f"Found {len(faces)} faces")</code></pre>

            <h3>4.6.2 Template Matching</h3>
            <pre><code># Load template
template = cv2.imread('template.jpg', 0)
w, h = template.shape[::-1]

# Perform template matching
result = cv2.matchTemplate(gray_image, template, cv2.TM_CCOEFF_NORMED)

# Get locations where matching is above threshold
threshold = 0.8
locations = np.where(result >= threshold)

# Draw rectangles
image_matched = image.copy()
for pt in zip(*locations[::-1]):
    cv2.rectangle(image_matched, pt, (pt[0] + w, pt[1] + h), (0, 255, 0), 2)</code></pre>

            <h2>4.7 Integrasi OpenCV dengan Deep Learning</h2>

            <h3>4.7.1 Preprocessing untuk CNN</h3>
            <pre><code>import tensorflow as tf

def preprocess_for_cnn(image_path, target_size=(224, 224)):
    # Load image dengan OpenCV
    image = cv2.imread(image_path)
    
    # Convert BGR to RGB
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # Resize
    image = cv2.resize(image, target_size)
    
    # Normalize
    image = image.astype('float32') / 255.0
    
    # Expand dims untuk batch
    image = np.expand_dims(image, axis=0)
    
    return image

# Contoh penggunaan
processed_image = preprocess_for_cnn('test.jpg')
# predictions = model.predict(processed_image)</code></pre>

            <h3>4.7.2 Loading Deep Learning Models di OpenCV</h3>
            <pre><code># OpenCV DNN module dapat load berbagai format model

# Load TensorFlow model
net = cv2.dnn.readNetFromTensorflow('frozen_graph.pb', 'graph.pbtxt')

# Load ONNX model
net = cv2.dnn.readNetFromONNX('model.onnx')

# Load Caffe model
net = cv2.dnn.readNetFromCaffe('deploy.prototxt', 'model.caffemodel')

# Contoh inference dengan YOLO
def detect_objects_yolo(image_path):
    # Load YOLO
    net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')
    
    # Load image
    image = cv2.imread(image_path)
    height, width = image.shape[:2]
    
    # Prepare blob
    blob = cv2.dnn.blobFromImage(
        image, 
        1/255.0, 
        (416, 416),
        swapRB=True,
        crop=False
    )
    
    # Set input
    net.setInput(blob)
    
    # Forward pass
    layer_names = net.getLayerNames()
    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]
    outputs = net.forward(output_layers)
    
    # Process outputs
    boxes = []
    confidences = []
    class_ids = []
    
    for output in outputs:
        for detection in output:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            
            if confidence > 0.5:
                center_x = int(detection[0] * width)
                center_y = int(detection[1] * height)
                w = int(detection[2] * width)
                h = int(detection[3] * height)
                
                x = int(center_x - w / 2)
                y = int(center_y - h / 2)
                
                boxes.append([x, y, w, h])
                confidences.append(float(confidence))
                class_ids.append(class_id)
    
    # Non-max suppression
    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)
    
    return boxes, confidences, class_ids, indices</code></pre>

            <h2>4.8 Video Processing dengan OpenCV</h2>

            <h3>4.8.1 Reading dan Writing Video</h3>
            <pre><code># Reading video
cap = cv2.VideoCapture('video.mp4')

# Get video properties
fps = cap.get(cv2.CAP_PROP_FPS)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

print(f"FPS: {fps}, Size: {width}x{height}")

# Video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter('output.mp4', fourcc, fps, (width, height))

# Process video frame by frame
while cap.isOpened():
    ret, frame = cap.read()
    
    if not ret:
        break
    
    # Process frame (contoh: convert to grayscale)
    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    colored_gray = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2BGR)
    
    # Write frame
    out.write(colored_gray)
    
    # Display frame
    cv2.imshow('Frame', colored_gray)
    
    # Press 'q' to quit
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
out.release()
cv2.destroyAllWindows()</code></pre>

            <h3>4.8.2 Webcam Real-time Processing</h3>
            <pre><code># Real-time face detection dari webcam
def realtime_face_detection():
    face_cascade = cv2.CascadeClassifier(
        cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
    )
    
    cap = cv2.VideoCapture(0)  # 0 = default webcam
    
    while True:
        ret, frame = cap.read()
        
        if not ret:
            break
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Detect faces
        faces = face_cascade.detectMultiScale(gray, 1.3, 5)
        
        # Draw rectangles
        for (x, y, w, h) in faces:
            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)
        
        # Display FPS
        cv2.putText(
            frame,
            f'Faces: {len(faces)}',
            (10, 30),
            cv2.FONT_HERSHEY_SIMPLEX,
            1,
            (0, 255, 0),
            2
        )
        
        cv2.imshow('Face Detection', frame)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()

# Uncomment untuk run:
# realtime_face_detection()</code></pre>

            <h2>4.9 OpenCV Best Practices</h2>

            <div class="tip-box">
                <h4>Tips untuk OpenCV yang Efektif:</h4>
                <ul>
                    <li><strong>Color Space:</strong> OpenCV menggunakan BGR, bukan RGB</li>
                    <li><strong>Performance:</strong> Gunakan NumPy operations untuk vectorization</li>
                    <li><strong>Memory:</strong> Release resources dengan <code>cap.release()</code> dan <code>cv2.destroyAllWindows()</code></li>
                    <li><strong>Preprocessing:</strong> Resize image ke ukuran yang konsisten sebelum processing</li>
                    <li><strong>GPU Acceleration:</strong> Gunakan cv2.cuda module untuk operasi GPU</li>
                    <li><strong>Error Handling:</strong> Selalu check <code>ret</code> value saat reading frames</li>
                </ul>
            </div>

            <h2>4.10 Resources dan Dokumentasi</h2>
            <ul>
                <li><strong>Official Documentation:</strong> https://docs.opencv.org/</li>
                <li><strong>Tutorials:</strong> https://docs.opencv.org/master/d9/df8/tutorial_root.html</li>
                <li><strong>PyImageSearch:</strong> https://pyimagesearch.com/ (excellent tutorials)</li>
                <li><strong>GitHub:</strong> https://github.com/opencv/opencv</li>
                <li><strong>OpenCV Courses:</strong> https://opencv.org/courses/</li>
            </ul>

            <div class="info-box">
                <h4>Langkah Selanjutnya:</h4>
                <p>Setelah memahami OpenCV untuk image processing, mari pelajari KerasCV yang mengintegrasikan computer vision dengan deep learning secara modern!</p>
            </div>
        </div>

        <!-- Page 5: KerasCV -->
        <div class="page" id="kerascv">
            <h1>KerasCV - Modern Computer Vision</h1>
            
            <h2>5.1 Pengenalan KerasCV</h2>
            <p>
                KerasCV adalah library modern dari Keras yang menyediakan building blocks modular untuk computer vision workflows. KerasCV menawarkan pre-trained models, augmentation layers, dan utilities yang production-ready dengan API yang consistent dan user-friendly.
            </p>

            <div class="info-box">
                <h4>Keunggulan KerasCV:</h4>
                <ul>
                    <li><strong>Modular Components</strong> - Layers dan models yang dapat dikombinasikan dengan mudah</li>
                    <li><strong>Pre-trained Models</strong> - State-of-the-art models siap pakai</li>
                    <li><strong>Advanced Augmentation</strong> - RandAugment, AutoAugment, MixUp, CutMix</li>
                    <li><strong>Production Ready</strong> - Optimized untuk training dan inference</li>
                    <li><strong>Modern Architectures</strong> - Vision Transformer, EfficientNet, YOLO</li>
                    <li><strong>Consistent API</strong> - Seamless integration dengan Keras dan TensorFlow</li>
                </ul>
            </div>

            <h2>5.2 Instalasi KerasCV</h2>
            <pre><code># Instalasi KerasCV
pip install keras-cv

# Instalasi dependencies
pip install tensorflow>=2.12.0

# Untuk GPU support
pip install tensorflow[and-cuda]

# Verifikasi instalasi
python -c "import keras_cv; print(keras_cv.__version__)"</code></pre>

            <div class="warning-box">
                <h4>Catatan Penting:</h4>
                <ul>
                    <li>KerasCV memerlukan TensorFlow 2.12 atau lebih baru</li>
                    <li>Beberapa fitur memerlukan keras-core atau keras 3.0</li>
                    <li>Untuk pre-trained weights, koneksi internet diperlukan saat first load</li>
                </ul>
            </div>

            <h2>5.3 Pre-trained Models dengan KerasCV</h2>

            <h3>5.3.1 Image Classification Models</h3>
            <pre><code>import keras_cv
import tensorflow as tf
from tensorflow import keras

# 1. EfficientNetV2 (Recommended)
model = keras_cv.models.EfficientNetV2B0(
    include_rescaling=True,
    include_top=True,
    num_classes=1000,
    weights='imagenet'
)

# 2. Vision Transformer
model = keras_cv.models.ViTB16(
    include_rescaling=True,
    include_top=True,
    num_classes=1000,
    weights='imagenet'
)

# 3. ConvNeXt (Modern CNN)
model = keras_cv.models.ConvNeXtTiny(
    include_rescaling=True,
    include_top=True,
    num_classes=1000,
enet'
)

# 4. DenseNet
model = keras_cv.models.DenseNet121(
    include_rescaling=True,
    include_top=True,
    num_classes=1000,
    weights='imagenet'
)

# Available models:
# - EfficientNetV2B0, B1, B2, B3, S, M, L
# - ViTB16, ViTB32, ViTL16, ViTL32, ViTH16
# - ConvNeXtTiny, Small, Base, Large, XLarge
# - ResNet50, ResNet101, ResNet152
# - DenseNet121, DenseNet169, DenseNet201</code></pre>

            <h3>5.3.2 Transfer Learning dengan KerasCV</h3>
            <pre><code>import keras_cv
import tensorflow as tf
from tensorflow import keras

# Load pre-trained backbone (tanpa classification head)
backbone = keras_cv.models.EfficientNetV2B0(
    include_rescaling=True,
    include_top=False,
    weights='imagenet',
    input_shape=(224, 224, 3)
)

# Freeze backbone untuk transfer learning
backbone.trainable = False

# Build custom model
inputs = keras.Input(shape=(224, 224, 3))
x = backbone(inputs, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
x = keras.layers.Dropout(0.5)(x)
outputs = keras.layers.Dense(10, activation='softmax')(x)

model = keras.Model(inputs, outputs)

# Compile
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

print(model.summary())

# Training
# history = model.fit(
#     train_dataset,
#     validation_data=val_dataset,
#     epochs=10
# )

# Fine-tuning: Unfreeze and train with lower learning rate
# backbone.trainable = True
# model.compile(
#     optimizer=keras.optimizers.Adam(1e-5),
#     loss='sparse_categorical_crossentropy',
#     metrics=['accuracy']
# )
# history_fine = model.fit(train_dataset, validation_data=val_dataset, epochs=5)</code></pre>

            <h2>5.4 Data Augmentation dengan KerasCV</h2>

            <h3>5.4.1 Basic Augmentation Layers</h3>
            <pre><code>import keras_cv

# Single augmentation layers
augmentation = keras.Sequential([
    # Geometric transformations
    keras_cv.layers.RandomFlip(mode="horizontal"),
    keras_cv.layers.RandomRotation(factor=0.1),
    keras_cv.layers.RandomZoom(height_factor=0.2, width_factor=0.2),
    keras_cv.layers.RandomTranslation(
        height_factor=0.1,
        width_factor=0.1
    ),
    
    # Color transformations
    keras_cv.layers.RandomBrightness(factor=0.2),
    keras_cv.layers.RandomContrast(factor=0.2),
    keras_cv.layers.RandomSaturation(factor=0.2),
    keras_cv.layers.RandomHue(factor=0.1),
    
    # Advanced augmentations
    keras_cv.layers.RandomShear(
        x_factor=0.2,
        y_factor=0.2
    ),
    keras_cv.layers.RandomCrop(
        height=200,
        width=200
    )
])

# Apply augmentation
augmented_images = augmentation(images, training=True)</code></pre>

            <h3>5.4.2 RandAugment - Auto Augmentation</h3>
            <pre><code># RandAugment secara otomatis memilih dan menerapkan augmentasi
rand_augment = keras_cv.layers.RandAugment(
    value_range=(0, 255),  # Pixel value range
    magnitude=0.5,         # Strength (0-1)
    augmentations_per_image=3,  # Jumlah augmentasi per image
    rate=1.0,              # Probability untuk apply
    magnitude_stddev=0.0
)

# Contoh pipeline lengkap
augmentation_pipeline = keras.Sequential([
    keras_cv.layers.Rescaling(scale=1./255),
    rand_augment,
])

# Apply ke dataset
def augment_data(image, label):
    image = augmentation_pipeline(image, training=True)
    return image, label

augmented_dataset = train_dataset.map(
    augment_data,
    num_parallel_calls=tf.data.AUTOTUNE
)</code></pre>

            <h3>5.4.3 MixUp dan CutMix</h3>
            <pre><code># MixUp: Blends dua images dan labels
mixup = keras_cv.layers.MixUp()

def apply_mixup(batch):
    images, labels = batch
    return mixup({'images': images, 'labels': labels})

# CutMix: Pastes cropped region dari satu image ke image lain
cutmix = keras_cv.layers.CutMix()

def apply_cutmix(batch):
    images, labels = batch
    return cutmix({'images': images, 'labels': labels})

# Combine dengan dataset
train_dataset = train_dataset.map(apply_mixup)
# atau
train_dataset = train_dataset.map(apply_cutmix)

# Random choice antara MixUp dan CutMix
class RandomMixupCutmix(keras.layers.Layer):
    def __init__(self):
        super().__init__()
        self.mixup = keras_cv.layers.MixUp()
        self.cutmix = keras_cv.layers.CutMix()
    
    def call(self, inputs):
        if tf.random.uniform([]) > 0.5:
            return self.mixup(inputs)
        else:
            return self.cutmix(inputs)</code></pre>

            <h2>5.5 Object Detection dengan KerasCV</h2>

            <h3>5.5.1 YOLO v8 untuk Object Detection</h3>
            <pre><code>import keras_cv

# Load pre-trained YOLO v8 model
model = keras_cv.models.YOLOV8Detector(
    num_classes=80,  # COCO dataset classes
    bounding_box_format='xyxy',
    backbone=keras_cv.models.YOLOV8Backbone.from_preset('yolo_v8_m_backbone_coco'),
    fpn_depth=2
)

# Load weights
model.load_weights('yolov8_coco.h5')

# Inference
def predict_objects(image_path):
    import cv2
    
    # Load dan preprocess image
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = tf.image.resize(image, (640, 640))
    image = tf.expand_dims(image, 0)
    
    # Predict
    predictions = model.predict(image)
    
    return predictions

# Visualize predictions
def visualize_detections(image, boxes, classes, scores):
    import matplotlib.pyplot as plt
    
    plt.imshow(image)
    ax = plt.gca()
    
    for box, cls, score in zip(boxes, classes, scores):
        if score > 0.5:  # Confidence threshold
            x1, y1, x2, y2 = box
            width = x2 - x1
            height = y2 - y1
            
            rect = plt.Rectangle(
                (x1, y1), width, height,
                fill=False,
                edgecolor='red',
                linewidth=2
            )
            ax.add_patch(rect)
            
            plt.text(
                x1, y1 - 5,
                f'Class {cls}: {score:.2f}',
                color='red',
                fontsize=12,
                bbox=dict(facecolor='white', alpha=0.8)
            )
    
    plt.axis('off')
    plt.show()</code></pre>

            <h3>5.5.2 RetinaNet untuk Object Detection</h3>
            <pre><code># RetinaNet dengan FPN
model = keras_cv.models.RetinaNet(
    num_classes=20,
    bounding_box_format='xywh',
    backbone=keras_cv.models.ResNet50Backbone(
        include_rescaling=True
    )
)

# Compile untuk training
model.compile(
    optimizer=keras.optimizers.Adam(1e-4),
    classification_loss='focal',
    box_loss='smoothl1'
)

# Training
# model.fit(train_dataset, validation_data=val_dataset, epochs=100)</code></pre>

            <h2>5.6 Image Segmentation dengan KerasCV</h2>

            <h3>5.6.1 Semantic Segmentation</h3>
            <pre><code># DeepLabV3+ untuk semantic segmentation
model = keras_cv.models.DeepLabV3Plus(
    num_classes=21,  # Pascal VOC classes
    backbone=keras_cv.models.ResNet50Backbone(
        include_rescaling=True
    )
)

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Prediction
def segment_image(image_path):
    image = tf.io.read_file(image_path)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, (512, 512))
    image = tf.expand_dims(image, 0)
    
    # Predict segmentation mask
    prediction = model.predict(image)
    mask = tf.argmax(prediction, axis=-1)[0]
    
    return mask

# Visualize segmentation
def visualize_segmentation(image, mask):
    import matplotlib.pyplot as plt
    
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    axes[0].imshow(image)
    axes[0].set_title('Original Image')
    axes[0].axis('off')
    
    axes[1].imshow(mask, cmap='tab20')
    axes[1].set_title('Segmentation Mask')
    axes[1].axis('off')
    
    # Overlay
    axes[2].imshow(image)
    axes[2].imshow(mask, alpha=0.5, cmap='tab20')
    axes[2].set_title('Overlay')
    axes[2].axis('off')
    
    plt.tight_layout()
    plt.show()</code></pre>

            <h2>5.7 Complete Training Pipeline dengan KerasCV</h2>

            <h3>Contoh End-to-End Image Classification</h3>
            <pre><code>import keras_cv
import tensorflow as tf
from tensorflow import keras
import tensorflow_datasets as tfds

# 1. Load Dataset
(train_ds, val_ds), info = tfds.load(
    'cifar10',
    split=['train', 'test'],
    as_supervised=True,
    with_info=True
)

# 2. Preprocessing dan Augmentation
BATCH_SIZE = 128
IMG_SIZE = 224

# Augmentation untuk training
train_augmentation = keras.Sequential([
    keras.layers.Rescaling(1./255),
    keras_cv.layers.RandAugment(
        value_range=(0, 1),
        magnitude=0.3,
        augmentations_per_image=2
    ),
    keras_cv.layers.RandomCutout(
        height_factor=0.1,
        width_factor=0.1
    )
])

# Preprocessing untuk validation
val_preprocessing = keras.Sequential([
    keras.layers.Rescaling(1./255)
])

def resize_and_augment(image, label):
    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
    image = train_augmentation(image, training=True)
    return image, label

def resize_and_preprocess(image, label):
    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
    image = val_preprocessing(image, training=False)
    return image, label

# Apply preprocessing
train_ds = train_ds.map(resize_and_augment, num_parallel_calls=tf.data.AUTOTUNE)
train_ds = train_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

val_ds = val_ds.map(resize_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)
val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# 3. Build Model dengan Transfer Learning
backbone = keras_cv.models.EfficientNetV2B0(
    include_rescaling=False,  # Already done in preprocessing
    include_top=False,
    weights='imagenet',
    input_shape=(IMG_SIZE, IMG_SIZE, 3)
)

inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
x = backbone(inputs, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
x = keras.layers.BatchNormalization()(x)
x = keras.layers.Dropout(0.5)(x)
outputs = keras.layers.Dense(10, activation='softmax')(x)

model = keras.Model(inputs, outputs)

# 4. Compile dengan Optimizer dan Learning Rate Schedule
lr_schedule = keras.optimizers.schedules.CosineDecay(
    initial_learning_rate=1e-3,
    decay_steps=1000
)

model.compile(
    optimizer=keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-4),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5)]
)

# 5. Callbacks
callbacks = [
    keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        patience=10,
        restore_best_weights=True
    ),
    keras.callbacks.ModelCheckpoint(
        'best_model.h5',
        monitor='val_accuracy',
        save_best_only=True
    ),
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-7
    ),
    keras.callbacks.TensorBoard(
        log_dir='./logs',
        histogram_freq=1
    )
]

# 6. Training
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=50,
    callbacks=callbacks
)

# 7. Fine-tuning
backbone.trainable = True

# Freeze early layers
for layer in backbone.layers[:-20]:
    layer.trainable = False

model.compile(
    optimizer=keras.optimizers.AdamW(1e-5, weight_decay=1e-4),
    loss='sparse_categorical_crossentropy',
=['accuracy']
)

history_fine = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,
    callbacks=callbacks
)

# 8. Evaluation
test_loss, test_acc = model.evaluate(val_ds)
print(f'Test accuracy: {test_acc:.4f}')

# 9. Save model
model.save('final_model.keras')</code></pre>

            <h2>5.8 Model Optimization dan Deployment</h2>

            <h3>5.8.1 Model Quantization</h3>
            <pre><code># TensorFlow Lite conversion dengan quantization
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# Post-training quantization
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# Quantize to float16
converter.target_spec.supported_types = [tf.float16]

# Convert
tflite_model = converter.convert()

# Save
with open('model_quantized.tflite', 'wb') as f:
    f.write(tflite_model)

print(f"Original model size: {os.path.getsize('final_model.keras') / 1024:.2f} KB")
print(f"Quantized model size: {os.path.getsize('model_quantized.tflite') / 1024:.2f} KB")</code></pre>

            <h3>5.8.2 ONNX Export</h3>
            <pre><code># Export to ONNX format untuk deployment
import tf2onnx

spec = (tf.TensorSpec((None, 224, 224, 3), tf.float32, name="input"),)

model_proto, _ = tf2onnx.convert.from_keras(
    model,
    input_signature=spec,
    opset=13,
    output_path="model.onnx"
)

print("Model successfully exported to ONNX format")</code></pre>

            <h2>5.9 KerasCV Best Practices</h2>

            <div class="tip-box">
                <h4>Tips untuk KerasCV yang Efektif:</h4>
                <ul>
                    <li><strong>Use Pre-trained Models:</strong> Transfer learning menghemat waktu dan meningkatkan accuracy</li>
                    <li><strong>Strong Augmentation:</strong> RandAugment + MixUp/CutMix untuk dataset kecil</li>
                    <li><strong>Proper Preprocessing:</strong> Pastikan preprocessing sama saat training dan inference</li>
                    <li><strong>Learning Rate Schedule:</strong> Cosine decay atau step decay untuk konvergensi lebih baik</li>
                    <li><strong>Mixed Precision:</strong> Gunakan <code>policy = tf.keras.mixed_precision.Policy('mixed_float16')</code></li>
                    <li><strong>Data Pipeline:</strong> Gunakan <code>prefetch</code> dan <code>cache</code> untuk performa optimal</li>
                    <li><strong>Fine-tuning Strategy:</strong> Freeze → Train head → Unfreeze → Fine-tune</li>
                </ul>
            </div>

            <h2>5.10 Comparison: KerasCV vs Alternatives</h2>

            <table>
                <tr>
                    <th>Feature</th>
                    <th>KerasCV</th>
                    <th>Torchvision</th>
                    <th>Detectron2</th>
                </tr>
                <tr>
                    <td><strong>Framework</strong></td>
                    <td>TensorFlow/Keras</td>
                    <td>PyTorch</td>
                    <td>PyTorch</td>
                </tr>
                <tr>
                    <td><strong>Ease of Use</strong></td>
                    <td>⭐⭐⭐⭐⭐</td>
                    <td>⭐⭐⭐⭐</td>
                    <td>⭐⭐⭐</td>
                </tr>
                <tr>
                    <td><strong>Pre-trained Models</strong></td>
                    <td>Modern architectures</td>
                    <td>Extensive collection</td>
                    <td>Detection focused</td>
                </tr>
                <tr>
                    <td><strong>Augmentation</strong></td>
                    <td>RandAugment, MixUp</td>
                    <td>Basic transforms</td>
                    <td>Detection specific</td>
                </tr>
                <tr>
                    <td><strong>Production Ready</strong></td>
                    <td>TF Serving</td>
                    <td>TorchServe</td>
                    <td>Research focused</td>
                </tr>
            </table>

            <h2>5.11 Resources dan Dokumentasi</h2>
            <ul>
                <li><strong>Official Documentation:</strong> https://keras.io/keras_cv/</li>
                <li><strong>GitHub Repository:</strong> https://github.com/keras-team/keras-cv</li>
                <li><strong>Examples:</strong> https://keras.io/examples/vision/</li>
                <li><strong>API Reference:</strong> https://keras.io/api/keras_cv/</li>
                <li><strong>Tutorials:</strong> https://www.tensorflow.org/tutorials</li>
            </ul>

            <div class="info-box">
                <h4>Kesimpulan Pembelajaran</h4>
                <p>
                    Selamat! Anda telah menyelesaikan panduan lengkap tentang Deep Learning dan Computer Vision. Anda telah mempelajari:
                </p>
                <ul>
                    <li>✅ <strong>TensorFlow</strong> - Framework fundamental untuk deep learning</li>
                    <li>✅ <strong>CNN</strong> - Arsitektur klasik untuk image processing</li>
                    <li>✅ <strong>Vision Transformer</strong> - State-of-the-art attention-based models</li>
                    <li>✅ <strong>OpenCV</strong> - Tools praktis untuk image manipulation</li>
                    <li>✅ <strong>KerasCV</strong> - Modern framework untuk production-ready CV applications</li>
                </ul>
                <p style="margin-top: 20px;">
                    <strong>Next Steps:</strong> Praktikkan dengan dataset nyata, ikuti competitions (Kaggle), dan build portfolio projects!
                </p>
            </div>

            <div style="text-align: center; margin-top: 50px; padding: 30px; background: #34495e; color: white; border-radius: 10px;">
                <h2>Terima Kasih!</h2>
                <p style="font-size: 1.2em;">Semoga panduan ini bermanfaat untuk perjalanan learning Anda.</p>
                <p style="margin-top: 20px;">Happy Learning & Coding!</p>
            </div>
        </div>

    </div>
    </div>
</body>
</html>
